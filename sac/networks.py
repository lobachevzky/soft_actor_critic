from typing import Tuple

import numpy as np
import tensorflow as tf
from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple

from sac.agent import AbstractAgent, NetworkOutput
from sac.utils import ArrayLike, Step, TrainStep


def mlp(inputs, layer_size, n_layers, activation):
    for i in range(n_layers):
        inputs = tf.layers.dense(inputs, layer_size, activation, name='fc' + str(i))
    return inputs


class MlpAgent(AbstractAgent):
    @property
    def seq_len(self):
        return None

    def network(self, inputs: tf.Tensor) -> NetworkOutput:
        return NetworkOutput(
            output=mlp(
                inputs=inputs,
                layer_size=self.layer_size,
                n_layers=self.n_layers,
                activation=self.activation),
            state=None)


class SACXAgent(AbstractAgent):
    @property
    def seq_len(self):
        return None

    @staticmethod
    def actor_network(inputs: tf.Tensor) -> NetworkOutput:
        l1 = tf.layers.dense(inputs, 200, tf.nn.elu)
        l2 = tf.contrib.layers.layer_norm(l1)
        l3 = tf.layers.dense(l2, 200, tf.nn.elu)
        return NetworkOutput(output=tf.layers.dense(l3, 100, tf.nn.elu), state=None)

    @staticmethod
    def critic_network(inputs: tf.Tensor) -> NetworkOutput:
        l1 = tf.layers.dense(inputs, 400, tf.nn.elu)
        l2 = tf.contrib.layers.layer_norm(l1)
        l3 = tf.layers.dense(l2, 400, tf.nn.elu)
        return NetworkOutput(output=tf.layers.dense(l3, 200, tf.nn.elu), state=None)

    def pi_network(self, o: tf.Tensor):
        with tf.variable_scope('pi'):
            return self.actor_network(o)

    def q_network(self, o: tf.Tensor, a: tf.Tensor, name: str,
                  reuse: bool = None):
        with tf.variable_scope(name, reuse=reuse):
            oa = tf.concat([o, a], axis=1)
            return tf.reshape(tf.layers.dense(self.critic_network(oa).output, 1, name='q'), [-1])

    def v_network(self, o: tf.Tensor, name: str, reuse: bool = None) -> tf.Tensor:
        with tf.variable_scope(name, reuse=reuse):
            return tf.reshape(tf.layers.dense(self.critic_network(o).output, 1, name='v'), [-1])


class LstmAgent(AbstractAgent):
    @property
    def seq_len(self):
        if self._seq_len:
            return self._seq_len
        return 8

    def __init__(self, batch_size: int, layer_size: int, device_num: int, **kwargs):
        self.batch_size = batch_size
        self.layer_size = layer_size
        with tf.device('/gpu:' + str(device_num)):
            state_args = tf.float32, [batch_size, layer_size]
            self.S = LSTMStateTuple(
                c=tf.placeholder(*state_args, name='C'),
                h=tf.placeholder(*state_args, name='H'))
            self.lstm = BasicLSTMCell(layer_size)
        super().__init__(
            batch_size=batch_size, layer_size=layer_size, device_num=device_num, **kwargs)
        self.initial_state = self.sess.run(self.lstm.zero_state(batch_size, tf.float32))
        assert np.shape(self.initial_state) == (2, batch_size, layer_size)
        assert self.S.c.shape == self.S.h.shape == (batch_size, layer_size)

    def network(self, inputs: tf.Tensor, reuse=False) -> tf.Tensor:
        split_inputs = tf.split(inputs, self.seq_len, axis=1)
        s = self.S
        for x in split_inputs:
            x = tf.squeeze(x, axis=1)
            outputs = NetworkOutput(*self.lstm(x, s))
        return outputs

    def state_feed(self, states):
        return dict(zip(self.S, states))

    def train_step(self, step: Step, feed_dict: dict = None) -> TrainStep:
        assert np.shape(step.s) == np.shape(self.initial_state)
        if feed_dict is None:
            feed_dict = {
                **self.state_feed(step.s),
                **{
                    self.O1: step.o1,
                    self.A: step.a,
                    self.R: np.array(step.r) * self.reward_scale,
                    self.O2: step.o2,
                    self.T: step.t
                }
            }
        return super().train_step(step, feed_dict)

    def q_network(self, o: tf.Tensor, a: tf.Tensor, name: str, reuse: bool = None) \
            -> tf.Tensor:
        with tf.variable_scope(name, reuse=reuse):
            o = self.network(o).output
            oa = tf.concat([o, a], axis=1)
            return tf.reshape(tf.layers.dense(oa, 1, name='q'), [-1])

    def get_actions(self, o: ArrayLike, sample: bool = True, state=None) \
            -> Tuple[np.ndarray, LSTMStateTuple]:
        assert len(np.shape(o)) == 1
        assert np.shape(state) == np.shape(self.initial_state)
        feed_dict = {**{self.O1: [[o]]}, **self.state_feed(state)}
        A = self.A_sampled1 if sample else self.A_max_likelihood
        return self.sess.run([A[0], self.S_new], feed_dict)


class MoEAgent(AbstractAgent):
    def __init__(self, n_networks, device_num, **kwargs):
        self.n_networks = n_networks
        super().__init__(device_num=device_num, **kwargs)

    @property
    def seq_len(self):
        return None

    def network(self, inputs: tf.Tensor):
        with tf.variable_scope('weights'):
            weights = mlp(
                inputs,
                self.layer_size,
                n_layers=self.n_layers - 1,
                activation=self.activation)
            weights = tf.layers.dense(weights, units=self.n_networks)
            weights = tf.nn.softmax(logits=weights, axis=-1)
        weights = tf.expand_dims(weights, axis=1)  # [batch, hidden, networks]

        def vote(i):
            with tf.variable_scope('vote' + str(i)):
                return mlp(inputs, self.layer_size, self.n_layers - 1, self.activation)

        h = tf.stack(
            values=list(map(vote, range(self.n_networks))),
            axis=-1)  # [batch, hidden, networks]
        h = tf.reduce_sum(h * weights, axis=2)
        output = tf.layers.dense(h, units=self.layer_size, name='output')
        return NetworkOutput(output=output, state=weights)
